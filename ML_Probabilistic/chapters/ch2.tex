\section{Probability}

\exercise[]{
\textbf{
My neighbor has two children. Assuming that the gender of a child is
like a coin flip, it is most likely, a priori, that my neighbor has one boy
and one girl, with probability 1/2. The other possibilities, two boys
or two girls, have probabilities 1/4 and 1/4. \\ \\
a. Suppose I ask him whether he has any boys, and he says yes. What is the
probability that one child is a girl? \\ \\
b. Suppose instead that I happen to see one of his children run by, and it is
a boy. What is the probability that the other child is a girl?}

a. Let $G$ represent one girl, and $B$ represent one boy. Since the neighbor
has two children, we can state the entire sample space:

$$S = \{BB, BG, GB, GG\}$$

When the neighbor answers the question, this changes our beliefs about the
other child. Using Bayes' theorem:

\begin{align}
    P(G = 1 | B \geq 1) & = \frac{P(B \geq 1 | G = 1) P(G = 1)}{P(B \geq 1)} \\
                        & = \frac{2/2 \times 1/2}{3/4} \\
                        & = \frac{2}{3}
\end{align}

b. If we instead happen to see one of his children, this is a different way
of looking at the problem. In this situation, learning the gender of one child
tells us nothing about the gender of the other child. Therefore, the gender of
the second child is a coin flip, $1/2$.

}

\exercise[]{
\textbf{
Suppose a crime has been committed. Blood is found at the scene for which there
is no innocent explanation. It is of a type which is present in 1\% of the
population. \\ \\
a. The prosecutor claims: "There is a 1\% chance that the defendant would have
the crime blood type if he were innocent. Thus there is a 99\% chance that he
is guilty". This is known as the prosecutor's fallacy. What is wrong with this
argument? \\ \\
b. The defender claims: "The crime occurred in a city of 800,000 people. The
blood type would be found in approximately 8000 people. The evidence has
provided a probability of just 1 in 8000 that the defendant is guilty, and
thus has no relevance". This is known as the defender's fallacy. What is wrong
with this argument?}

a. The defendant sharing the blood type does not mean that the defendant
himself has a 99\% probability of being guilty, just that he shares the same
blood type as the guilty party, just like he shares the same blood type with
1\% of the population. In a large enough city, there would be a large number
of people fitting this description in a small geographical radius.

b. This statement assumes that the defendent is just as guilty (or just as
non-guilty) as anyone else in that group of 8000 people. If there truly is
no other evidence to tie this defendent to this crime, then that may be so,
but if there were any other evidence (drives a similar car as the criminal,
lives in the same area, or frequents the same locations), the probability
that the defendant is guilty could be much higher.
}

\exercise[]{
\textbf{Show that the variance of a sum is $Var[X + Y] = Var[X] + Var[Y] + 2Cov[X,Y]$,
where $Cov[X, Y]$ is the covariance between $X$ and $Y$.}

\begin{align}
    & Var[X] + Var[Y] + 2Cov[X, Y] = E[(X - \mu_x)^2] + E[(Y - \mu_y)^2] + 2E[(X - \mu_x)(Y - \mu_y)] \\
    & = E[X^2 - 2X\mu_x + \mu_x^2] + E[Y^2 - 2Y\mu_y + \mu_y^2] + E[2XY - 2X\mu_y - 2Y\mu_x + 2\mu_x\mu_y] \\
    & = E[X^2 - 2X\mu_x + \mu_x^2 + Y^2 - 2Y\mu_y + \mu_y^2 + 2XY - 2X\mu_y - 2Y\mu_x + 2\mu_x\mu_y] \\
    & = E[X^2 + 2XY - 2X(\mu_x + \mu_y) + Y^2 - 2Y(\mu_x + \mu_y) + 2\mu_x\mu_y]
\end{align}

Note that $E(X + Y) = E(X) + E(Y) = \mu_x + \mu_y = \mu_{xy}$. Given this,

\begin{align}
    & E[X^2 + 2XY - 2X(\mu_x + \mu_y) + Y^2 - 2Y(\mu_x + \mu_y) + 2\mu_x\mu_y] \\
    & = E[X^2 + 2XY - 2X\mu_{xy} + Y^2 - 2Y\mu_{xy} + 2\mu_x\mu_y] \\
    & = E[(X + Y - \mu_{xy})^2] \\
    & = Var[X + Y]
\end{align}

}

\exercise[]{
\textbf{After your yearly checkup, the doctor has bad news and good news. The
bad news is that you tested positive for a serious disease, and that the test
is 99\% accurate (i.e., the probability of testing positive given that you
have the disease is 0.99, as is the probability of testing negative given that
you don't have the disease). The good news is that this is a rare disease,
striking only one in 10,000 people. What are the chances that you actually have
the disease? (Show your calculations as well as giving the final result.)}

Since the test is 99\% accurate, we know that $P(Y | D) = P(N | ~D) = 0.99$,
where $Y$ means a positive test result, $N$ a negative test result, $D$ means
you have the disease, and $~D$ means you do not have the disease. We also know
the prior probability of having the disease: $P(D) = 0.0001$. Using Bayes'
rule:

\begin{align}
    P(D | Y) & = \frac{P(Y | D)P(D)}{P(Y)} \\
             & = \frac{0.99 \times 0.0001}{P(Y | D)P(D) + P(Y | ~D)P(~D)} \\
             & = \frac{0.000099}{(0.99 \times 0.0001) + (0.01 \times 0.9999)} \\
             & = \frac{0.000099}{0.000099 + 0.009999} \\
             & = 0.0098
\end{align}

So, there's about a 1\% chance that you have the disease even though you tested
positive for it.

}

\exercise[]{
\textbf{Solve the Monty Hall problem.}

The key here is that the host will never open a door that has the car
in it. So from that sense, the contestant does not disturb the original
distribution, but it does give you additional information.

Let's say you originally pick the door with the car. Under these circumstances,
which door the host will open is uniform ($1/2$). In this situation, it is
worse for you to switch, and you'll only find yourself in this situation if you
pick the door correctly the first time ($1/3$ chance).

Let's say you did not originally pick the door with the car. Under these
circumstances, the door that the host will open is completely deterministic.
Two doors will remain, one with the car, and the host will never choose that
one. In this situation, it is better for you to switch, because you'll be
gauranteed a car.

So, when you guess correctly the first time and switch, you're guaranteed to
lose the car. If you guess incorrectly the first time and switch, you're
guaranteed to win the car. The probability of guessing correctly the first
time is $1/3$, so switching will give you a winning probability of $2/3$.
}

\exercise[]{
\textbf{a. Let $H \in \{1, ... , K\}$ be a discrete random variable, and let
$e_1$ and $e_2$ be the observed values of two other random variables $E_1$
and $E_2$. Suppose we wish to calculate the vector \\
$$\vec{P}(H|e_1, e_2) = (P(H = 1 | e_1, e_2), ..., P(H=K|e_1, e_2))$$ \\
Which of the following sets of numbers are sufficient for the calculation? \\
i. $P(e_1, e_2), P(H), P(e_1|H), P(e_2|H)$ \\
ii. $P(e_1, e_2), P(H), P(e_1, e_2|H)$ \\
iii. $P(e_1|H), P(e_2|H), P(H)$ \\
b. Now suppose we now assume $E_1 \perp E_2|H$ (i.e., $E_1$ and $E_2$ are
conditionally independent given $H$). Which of the above 3 sets are sufficient
now?}

a. Let's use Bayes' Theorem to decompose this a bit:

\begin{align}
    P(H | e_1, e_2) & = \frac{P(e_1, e_2, H)}{P(e_1, e_2)} \\
                    & = \frac{P(e_1, e_2 | H)P(H)}{P(e_1, e_2)} \\
                    & = \frac{P(e_1 | e_2, H)P(e_2)P(H)}{P(e_1, e_2)}
\end{align}

From this, we can see that \textbf{ii} is sufficient to solve this.

b. If we assume that they are now conditionally independent, then this allows
\textbf{i} to be sufficient as well. Note that \textbf{iii} is still not
sufficient, but if we know that $E_1$ and $E_2$ were unconditionally
independent, this would be sufficient as well.

}

\exercise[]{
\textbf{Show that pairwise independence between all pairs of variables does
not necessarily imply mutual independence. It suffices to give a
counterexample.}

}

\exercise[]{
\textbf{In the text we said $X \perp Y|Z$ iff $p(x,y|z) = p(x|z)p(y|z)$ for
all $x$, $y$, $z$ such that $p(z) > 0$. Now prove the following alternative
definition: $X \perp Y|Z$ iff there exist functions $g$ and $h$ such that
$p(x, y | z) = g(x, z)h(y, z)$ for all $x$, $y$, $z$ such that $p(z) > 0$.}

For this to be true, $g(x, z)h(y, z) = p(x|z)p(y|z)$ So by computing the
marginal probabilities, we can see if they are equivalent.

\begin{align}
    p(x|z) & = \sum_{y} p(x, y | z) \\
           & = \sum_{y} g(x, z)h(y, z) \\
           & = g(x, z)\sum_{y} h(y, z)
\end{align}

and therefore

$$\sum_{y}h(y, z) = \frac{p(x | z)}{g(x, z)}$$

Similarly we can find that

$$\sum_{x}g(x, z) = \frac{p(y | z)}{h(y, z)}$$

and we can note that

$$\sum_{x}\sum_{y} p(x, y | z) = \sum_{x}g(x, z) \sum_{y} h(y, z) = 1$$

so therefore

\begin{align}
    \sum_{x}\sum_{y} p(x, y | z) & = \sum_{x}g(x, z) \sum_{y}h(y, z) \\
                                 & = \frac{p(y|z)}{h(y,z)} \frac{p(x|z)}{g(x,z)} \\
                                 & = 1
\end{align}

which leads to

\begin{align}
    & 1 = \frac{p(y|z)}{h(y,z)} \frac{p(x|z)}{g(x,z)} \\
    & \frac{g(x,z)}{p(x|z)} = \frac{p(y|z)}{h(y,z)} \\
    & p(x|z)p(y|z) = g(x,z)h(y,z)
\end{align}

}

\exercise[]{
\textbf{\\
a. True or false? $(X\perp W|Z,Y)\wedge(X\perp Y|Z)\Rightarrow(X\perp Y,W|Z)$ \\
b. True or false? $(X\perp Y|Z)\wedge(X\perp Y|W)\Rightarrow(X\perp Y|Z,W)$}

a. Blowing out the component parts:

\begin{align}
    & (X\perp W|Z,Y) \Leftrightarrow p(X, W | Z, Y) = p(X | Z, Y)p(W | Z, Y) \\
    & (X\perp Y|Z) \Leftrightarrow p(X,Y|Z) = p(X|Z)p(Y|Z) \\
    & (X\perp Y,W|Z) \Leftrightarrow p(X,Y,W|Z) = p(X|Z)p(Y|Z)p(W|Z)
\end{align}

so we can see if we can recreate the righthand side using what we know:

P(A,B) = P(A|B)P(B)
P(A|B) = P(A,B)/P(B)

P(A,B|C,D) = P(A,B,D|C)/P(D)

\begin{align}
    p(X,Y,W|Z) & = p(X,W|Z,Y)p(Y) \\
               & = p(X|Z,Y)p(W|Z,Y)p(Y) \\
               & = \frac{p(X,Y|Z)}{p(Y)}p(W|Z,Y)p(Y) \\
               & = p(X|Z)p(Y|Z)p(W|Z)
\end{align}

b. Blowing out the component parts:

\begin{align}
    & (X\perp Y|Z) \Leftrightarrow p(X,Y|Z) = p(X|Z)p(Y|Z) \\
    & (X\perp Y|W) \Leftrightarrow p(X,Y|W) = p(X|W)p(Y|W) \\
    & (X\perp Y|Z,W) \Leftrightarrow p(X,Y|Z,W) = p(X|Z,W)p(Y|Z,W)
\end{align}

Note that $W$ is in the conditioning set for the equation on the righthand
side. There is no way to remove this from the conditioning set while also
removing it from the lefthand side. Therefore this is false.
}

\exercise[]{
\textbf{Given the Gamma density, show that the inverse Gamma is a Gamma with
a change of variables to $Y = 1/X$.}

The Gamma is given by:

$$Ga(x|a,b) = \frac{b^a}{\tau(a)}x^{a-1}e^{-xb}$$

and the Inverse Gamma is given by:

$$IG(x|a,b) = \frac{b^a}{\tau(a)}x^{-(a+1)}e^{-b/x}$$

from these, it is easy to show that:

\begin{align}
    Ga(\frac{1}{x}|a,b) & = \frac{b^a}{\tau(a)}(\frac{1}{x})^{a-1}e^{-b/x} \\
        & = \frac{b^a}{\tau(a)}x^{-1\times (a-1)}e^{-b/x} \\
        & = \frac{b^a}{\tau(a)}x^{-(a+1)}e^{-b/x} \\
        & = IG(x|a,b)
\end{align}
}

\exercise[]{
\textbf{Show that the normalization constant for the Gaussian distribution is
equal to $Z = \sigma \sqrt{2 \pi}$.}

This is essentially just computing the integral

\begin{align}
    Z^2 & = \int_{0}^{2\pi}\int_{0}^{\infty}r\,exp(-\frac{r^2}{2\sigma^2})drd\theta \\
        & = 2\pi \int_{0}^{\infty}r\,exp(-\frac{r^2}{2\sigma^2})dr \\
        & = 2\pi \sigma^2 e^{-\frac{r^2}{2\sigma^2}} |_0^{\infty} \\
        & = 2\pi \sigma^2
\end{align}

Therefore $Z = \sigma \sqrt{2\pi}$.

}

\exercise[]{
\textbf{Show that $$I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$}

To show this, we must use the definitions of KL divergence, entropy,
and conditional entropy. I will show the definitions here for clarity.

\begin{align}
    H(X) & = - \sum_{k=1}^{K} p(X = k) log_2 p(X = k) \\
    KL(X || Y) & = -H(X) + H(X, Y) \\
    H(Y | X) & = \sum_{x} p(x) H(Y | X = x) \\
             & = H(X, Y) - H(X) \\
    I(X, Y) & = KL(p(X, Y) || p(X)p(Y))
\end{align}

With these definitions, we can show that

\begin{align}
    I(X, Y) & = KL(p(X,Y) || p(X)p(Y)) \\
            & = -H(p(X,Y)) + H(p(X,Y), p(X),pY()) \\
            & = \sum_{x \in X} \sum_{y \in Y} p(x,y)log\,p(x,y) -
                \sum_{x \in X} \sum_{y \in Y} p(x,y) log\,p(x)p(y) \\
            & = \sum_{x \in X} \sum_{y \in Y} p(x,y)(log\,p(y|x) + log\,p(x)) -
                \sum_{x \in X} \sum_{y \in Y} p(x,y)(log\,p(x) + log\,p(y)) \\
            & = -H(Y|X) + \sum_{x \in X} \sum_{y \in Y} p(x,y)log\,p(x) -
                \sum_{x \in X} \sum_{y \in Y} p(x,y)(log\,p(x) + log\,p(y)) \\
            & = -H(Y|X) + \sum_{x \in X} p(x)log\,p(x) -
                \sum_{x \in X} p(x)log\,p(x) +
                \sum_{y \in Y} p(y)log\,p(y) \\
            & = -H(Y|X) + \sum_{y \in Y} p(y)log\,p(y) \\
            & = -H(Y|X) + H(Y) \\
            & = H(Y) - H(Y|X)
\end{align}
}

\exercise[]{
\textbf{Evaluate $I(X_1, X_2)$ where $X$ has a bivariate normal distribution.
Evaluate $I(X_1, X_2)$ at $\rho = -1$, $\rho = 0$, $\rho = 1$.}

The entropy for both multidimensional and single dimensional Gaussians is
defined by:

\begin{align}
    h(\mathbf{X}) & = \frac{1}{2} log_2 [ (2\pi e)^d \,det\, \Sigma ] \\
    h(X) & = \frac{1}{2} log_2 [ 2\pi e \sigma^2 ]
\end{align}

Using these, we can compute:

\begin{align}
    I(X_1,X_2) & = H(X_1) - H(X_1|X_2) \\
& = H(X_1) - H(X_2,X_1) + H(X_2) \\
& = log_2[2\pi e \sigma^2] - H(X_2,X_1) \\
& = log_2[2\pi e] + log_2[\sigma^2] - H(X_2,X_1) \\
& = C + log_2[\sigma^2] - H(X_2,X_1) \\
& = C + 2log_2[\sigma] - \frac{1}{2}log_2[(2\pi e)^2\,det\, \Sigma] \\
& = C + 2log_2[\sigma] - \frac{1}{2}[2C + log_2[det\, \Sigma]] \\
& = C + 2log_2[\sigma] - C - \frac{1}{2}log_2[\sigma^4(1-\rho^2)] \\
& = 2log_2[\sigma] - \frac{1}{2}log_2[\sigma^4(1-\rho^2)]
\end{align}

where $C = log_2[2\pi e]$. Now we can plug in various values for $\rho$. When
$\rho = -1$,

\begin{align}
    I(X_1,X_2) & = 2log_2[\sigma] - log_2[\sigma^4] \\
               & = 2log_2[\sigma] - 4log_2[\sigma^4] \\
               & = -2log_2[\sigma]
\end{align}

When $\rho = 0$,

\begin{align}
    I(X_1,X_2) & = 2log_2[\sigma] - \frac{1}{2}log_2[\sigma^4] \\
               & = 2log_2[\sigma] - 2log_2[\sigma] \\
               & = 0
\end{align}

When $\rho = 1$,

$$I(X_1,X_2) = 2log_2[\sigma]$$

}

\exercise[]{
\textbf{Let $X$ and $Y$ be discrete random variables which are identically
distributed (so $H(X) = H(Y)$) but not necessarily independent. Define}

$$r = 1 - \frac{H(Y|X)}{H(X)}$$

\textbf{a. Show that $r = \frac{I(X,Y)}{H(X)}$ }

\begin{align}
    \frac{I(X,Y)}{H(X)} & = \frac{H(Y) - H(Y|X)}{H(X)} \\
                        & = \frac{H(X) - H(Y|X)}{H(X)} \\
                        & = 1 - \frac{H(Y|X)}{H(X)}
\end{align}

\textbf{b. Show that $0 \leq r \leq 1$}

\begin{align}
    r & = 1 - \frac{H(Y|X)}{H(X)} \\
      & = 1 - \frac{H(X,Y) - H(X)}{H(X)} \\
      & = 1 - H(X,Y) = 1 - H(X)
\end{align}

\textbf{c. When is $r = 0$?}

$r = 0$ when the entropy of $X$ (or equivalently $Y$) $ = 1$.

\textbf{d. When is $r = 1$?}

$r = 1$ when the entropy of $X$ (or equivalently $Y$) $ = 0$. This happens
when $X$ and $Y$ is completely deterministic.

}

\exercise[]{
\textbf{Let $p(x)$ be the empirical distribution and $q(x|\theta)$ be a model.
Show that $argmin_q\, KL(p || q)$ is obtained by $q(x) = q(x|\hat{\theta})$,
where $\theta$ is the MLE.}

Since

$$KL(p || q) = \sum_{k} p_k log\,p_k - \sum_{k} p_k log\,q_k$$

minimizing this is equivalent to maximizing

\begin{align}
    max_q\, -KL(p || q) & = -\sum_{k} p_k log\,p_k + \sum_{k} p_k log\,q_k
                        & = \sum_{k} p_k log\,q_k
\end{align}

This is the maximum likelihood equation.

}

\exercise[]{
\textbf{Derive the mean, mode, variance of $\theta ~ Beta(a,b)$}

The pdf of the beta distribution is given by

$$\frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$$

Note that $B(\alpha, \beta)$ is a normalization constant, and in order
to derive the moments of the distribution we will be using MLE, so for
our purposes, we can ignore this constant.

The mode is defined as the peak of the distribution:

\begin{align}
    max\,x^{\alpha-1}(1-x)^{\beta-1}& \equiv max\,log[x^{\alpha-1}(1-x)^{\beta-1}] \\
    & = max\,(\alpha-1)log(x) + (\beta-1)log(1 - x)
\end{align}

by taking the derivative and setting it to $0$, we get:

\begin{align}
    0 & = \frac{\alpha - 1}{x} + \frac{\beta - 1}{1 - x} \\
    \frac{-\beta + 1}{-1 + x} & = \frac{\alpha - 1}{x} \\
    x(-\beta + 1) & = (\alpha - 1)(x - 1) \\
    -\beta x + x & = -\alpha x - \alpha - x + 1 \\
    \alpha x - \beta x + 2x & = \alpha - 1 \\
    x & = \frac{\alpha - 1}{\alpha + \beta - 2}
\end{align}

The mean of the distribution is defined by:

\begin{align}
    E[x] & = \int_0^1 xp(x)dx \\
         & = \int_0^1 x\frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}dx \\
         & = \frac{\alpha}{\alpha + \beta}
\end{align}

The variance of the distribution is defined by:

\begin{align}
    Var(X) & = E[(X -\mu)^2] = E[X^2] - E[X]^2 \\
           & = E[X^2] - (\frac{\alpha}{\alpha + \beta})^2 \\
           & = \int_0^1 x^2p(x)dx - (\frac{\alpha}{\alpha + \beta})^2 \\
& = \int_0^1 x^2\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}dx -
    (\frac{\alpha}{\alpha + \beta})^2 \\
& = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\end{align}

}

\exercise[]{
\textbf{Suppose $X$, $Y$ are two points sampled independently and uniformly
at random from the interval $[0,1]$. What is the expected location of the
left most point?}

\begin{align}
    E[min(X,Y)] & = \int_0^1 \int_0^1 min(X,Y)p(X,Y)dxdy \\
                & = \int_0^1 \int_0^1 min(X,Y)p(X)p(Y)dxdy \\
                & = \int_0^1 \int_0^1 min(X,Y)dxdy \\
                & = \frac{1}{2} \int_0^1 \int_0^1 X + Y - |X - Y| dxdy \\
                & = \frac{1}{3}
\end{align}

}

\exerciseshere
