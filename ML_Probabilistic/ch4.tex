\section{Gaussian Models}

\exercise[]{
\textbf{Let $X \sim U(−1, 1)$ and $Y = X^2$. Clearly $Y$ is dependent on $X$
(in fact, $Y$ is uniquely determined by $X$). However, show that
$\rho(X,Y) = 0$. Hint: if $X \sim U(a, b)$ then $E[X]=(a + b)/2$ and
$var[X]=(b − a)^2/12$.}

Let's plug things into the definition of correlation:

\begin{align}
    \rho(X,Y) & = \frac{cov(X,Y)}{\sigma_X\sigma_Y} \\
              & = \frac{E[XY] - E[X]E[Y]}{\sigma_X\sigma_Y} \\
              & = \frac{E[X^3] - E[X]E[X^2]}{\sigma_X\sigma_{X^2}}
\end{align}

Note that to show this equals $0$, we just have to show that the numerator
is equal to $0$. To do this, we will compute each term:

$$E[X^3] = \frac{1}{2} \int_{-1}^{1} u^3p(u)du = 0$$

$$E[X^2] = \frac{1}{2} \int_{-1}^{1} u^2p(u)du = \frac{1}{3}$$

$$E[X] = \frac{-1 + 1}{2} = 0$$

So we have

\begin{align}
    \rho(X,Y) & = \frac{E[X^3] - E[X]E[X^2]}{\sigma_X\sigma_{X^2}} \\
              & = \frac{0 - 0\times \frac{1}{3}}{\sigma_X\sigma_{X^2}} \\
              & = 0
\end{align}

}

\exercise[]{
\textbf{Let $X \sim N(0,1)$ and $Y = WX$, where $p(W = −1) = p(W = 1) = 0.5$.
It is clear that $X$ and $Y$ are not independent, since $Y$ is a function of $X$. \\
a. Show that $Y \sim N(0,1)$.}

So, $W$ randomly changes the sign half the time on $X$. Thus,

$$Y \sim \frac{1}{2} N(0,1) - \frac{1}{2} N(0,1)$$

Let's write the distribution out for this:

\begin{align}
    p(Y|\mu,\sigma^2) & = \frac{1}{\sqrt{2\pi \sigma^2}}
                          e^{-\frac{(WX-\mu)^2}{2\sigma^2}} \\
    & = \frac{1}{2\sqrt{2\pi \sigma^2}}e^{-\frac{(-X-\mu)^2}{2\sigma^2}} +
        \frac{1}{2\sqrt{2\pi \sigma^2}}e^{-\frac{(X-\mu)^2}{2\sigma^2}} \\
    & = \frac{1}{2\sqrt{2\pi}}e^{-\frac{(-X)^2}{2}} +
        \frac{1}{2\sqrt{2\pi}}e^{-\frac{X^2}{2}} \\
    & = \frac{1}{2\sqrt{2\pi}}\left ( e^{-\frac{X^2}{2}} + e^{-\frac{X^2}{2}} \right ) \\
    & = \frac{1}{2\sqrt{2\pi}}\left ( 2e^{-\frac{X^2}{2}} \right ) \\
    & = \frac{1}{\sqrt{2\pi}}e^{-\frac{X^2}{2}} \\
    & = N(0,1)
\end{align}

\textbf{b. Show that $cov[X,Y]=0$.}

\begin{align}
    cov[X,Y] & = E[XY] - E[X]E[Y] \\
             & = E[E[XY|W]] - E[X]E[WX] \\
             & = \frac{1}{2}E[X^2] + \frac{1}{2}E[-X^2] - E[X]E[WX] \\
             & = \frac{1}{2}E[X^2] + \frac{1}{2}E[X^2] - E[X]E[WX] \\
             & = E[X^2] - E[X](\frac{1}{2}E[^2]+\frac{1}{2}E[-X^2]) \\
             & = E[X^2] - E[X^2] \\
             & = 0
\end{align}

}

\exercise[]{
\textbf{Prove that $-1 \leq \rho(X,Y) \leq 1$}

$$\rho(X,Y) = \frac{cov(X,Y)}{\sigma_X\sigma_Y}$$

This is trivial to prove with the Cauchy-Swartz inequality which states
that

$$|cov(X,Y)| \leq \sqrt{\sigma_X^2\sigma_Y^2}$$

because for $\rho(X,Y)$ to be $> 1$ or $< -1$, then
$|cov(X,Y)| > \sqrt{\sigma_X^2\sigma_Y^2}$, which is false.

}

\exercise[]{
\textbf{Show that, if $Y=aX+b$ for some parameters $a>0$ and $b$, then
$\rho(X,Y)=1$. Similarly show that if $a<0$, then $\rho(X,Y)=-1$.}

\begin{align}
    \rho(X,Y) & = \frac{cov(X,Y)}{\sigma_X\sigma_Y} \\
    & = \frac{E[(X-E[X])(Y-E[Y])]}{\sqrt{E[(X-E[X])^2]E[(Y-E[Y])^2]}} \\
\end{align}

Note that the quantity $(Y-E[Y])$ can be written as

\begin{align}
    Y-E[Y] & = aX+b-E[aX+b] \\
           & = aX+b-b-aE[X] \\
           & = a(X-E[X])
\end{align}

Plugging this, we get

\begin{align}
    \rho(X,Y) & = \frac{aE[(X-E[X])(X-E[X])]}{|a|\sqrt{E[(X-E[X])^2]E[(X-E[X])^2]}} \\
              & = \frac{E[(X-E[X])(X-E[X])]}{E[(X-E[X])^2]} \\
              & = \frac{E[(X-E[X])^2]}{E[(X-E[X])^2]} \\
              & = 1
\end{align}

If $a<0$, then this changes to

\begin{align}
    \rho(X,Y) & = \frac{aE[(X-E[X])^2]}{|a|\sqrt{E[(X-E[X])^2]E[(X-E[X])^2]}} \\
              & = -\frac{E[(X-E[X])^2]}{E[(X-E[X])^2]} \\
              & = -1
\end{align}

}

\exercise[]{
\textbf{Derive the normalization constant for multivariate Gaussian.}

We are trying to show that

$$(2\pi)^{D/2}|\Sigma|^{1/2} = \int exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))dx$$

Using eigenvalue decomposition on $\Sigma = U\Lambda U^T$, we can write this as

\begin{align}
    (2\pi)^{D/2}|\Sigma|^{1/2} & =
        \int exp(-\frac{1}{2}(x-\mu)^T U\Lambda^{-1} U^T(x-\mu))dx \\
    & = \int exp(-\frac{1}{2}u^T \Lambda^{-1} u) du \\
    & = \int exp(-\frac{1}{2} \sum_d \frac{u_d^2}{\lambda_d}) du \\
    & = \prod_{i=1}^D \int exp(-\frac{u_i^2}{2 \lambda_i}) du
\end{align}

Note that this is the product of single dimensional Gaussians. We know that
$\int exp(-\frac{u^2}{2\sigma^2}) = \sqrt{2\pi \sigma^2}$, and so we can rewrite
this expression as

\begin{align}
    \prod_{i=1}^D \int exp(-\frac{u_i^2}{2 \lambda_i}) du & =
        \prod_{i=1}^D \sqrt{2\pi \lambda_i} \\
    & = (2\pi)^{D/2} \prod_{i=1}^D \lambda_i^{1/2} \\
    & = (2\pi)^{D/2} |\Sigma|^{1/2}
\end{align}

}

\exercise[]{
\textbf{Derive the pdf of the bivariate Guassian with $\Sigma$ given.}

Note that

\begin{align}
    \Sigma^{-1} = 
    \frac{1}{\sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2}
    \begin{bmatrix}
        \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\ 
        -\rho \sigma_1 \sigma_2 & \sigma_1^2 
    \end{bmatrix}
\end{align}

and

\begin{align}
    & (x-\mu)^T\Sigma^{-1}(x-\mu) = 
    \frac{1}{\sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2}
    \begin{bmatrix}
        x_1 - \mu_1 & x_2 - \mu_2
    \end{bmatrix}
    \begin{bmatrix}
        \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\ 
        -\rho \sigma_1 \sigma_2 & \sigma_1^2 
    \end{bmatrix}
    \begin{bmatrix}
        x_1-\mu_1 \\ x_2-\mu_2 
    \end{bmatrix} \\
    & = \frac{1}{\sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2}
    \begin{bmatrix}
        \sigma_1^2(x_1-\mu_1) + \rho \sigma_1 \sigma_2 (x_2 - \mu_n) &
        \rho \sigma_1 \sigma_2 (x_1 - \mu_1) + \sigma_2^2 (x_2 - \mu_n)
    \end{bmatrix}
    \begin{bmatrix}
        x_1-\mu_1 \\ x_2-\mu_2 
    \end{bmatrix} \\
    & = \frac{1}{\sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2}
      (x_1 - \mu_1) (\sigma_1^2 (x_1 - \mu_1) + \rho \sigma_1 \sigma_2
         (x_2 - \mu_2)) + (x_2 - \mu_2) (\rho \sigma_1 \sigma_2
         (x_1 - \mu_1) + \sigma_2^2 (x_2 - \mu_2)) \\
    & = \frac{1}{\sigma_1^2 \sigma_2^2 - \rho^2 \sigma_1^2 \sigma_2^2}
    \sigma_1^2 (x_1-\mu_1)^2 + 2 \rho \sigma_1 \sigma_2(x_1-\mu_1)(x_2-\mu_2)
        + \sigma_2^2 (x_2-\mu_2)^2 \\
    & = \frac{1}{1-\rho^2}\frac{\sigma_1^2 (x_1-\mu_1)^2}{\sigma_1^2 \sigma_2^2} +
        \frac{2 \rho \sigma_1 \sigma_2(x_1-\mu_1)(x_2-\mu_2)}
             {\sigma_1^2 \sigma_2^2} + \frac{\sigma_2^2 (x_2-\mu_2)^2}
             {\sigma_1^2 \sigma_2^2} \\
    & = \frac{1}{1-\rho^2}\left ( \frac{(x_1-\mu_1)^2}{\sigma_1^2} +
        \frac{(x_2-\mu_2)^2}{\sigma_2^2} + 2\rho \frac{(x_1-\mu_1)}{\sigma_1}
        \frac{(x_2-\mu_2)}{\sigma_2} \right )
\end{align}

We see that this is the quantity requested of us in the exercise.

}

\exercise[]{
\textbf{Compute the conditional probability distribution of the given
bivariate Gaussian.}

Note that the conditional probability of two Gaussians is a Gaussian. Also
the conditional probability distribution is given by

\begin{align}
    p(x_1|x_2) & = N(x_1|\mu_{1|2},\Sigma_{1|2}) \\
    \mu_{1|2}  & = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
    \Sigma_{1|2} & = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{align}

It simplifies things greatly that we are considering a bivariate Gaussian,
since $\Sigma_{jk}$ becomes a scalar. Thus, after plugging in to the equations
given in the problem,

\begin{align}
    p(x_2|x_1) & = N(x_2|\mu_{2|1},\Sigma_{2|1}) \\
    \mu_{2|1} &=\mu_2+\sigma_1\sigma_2(\rho \frac{\sigma_2}{\sigma_1}(x_1-\mu_1)) \\
    & = \mu_2 + \rho \sigma_2^2 (x_1 - \mu_1) \\
    \Sigma_{2|1} & = \sigma_1\sigma_2 \frac{\sigma_2}{\sigma_1} -
                     \sigma_1\sigma_2 \rho^2 \frac{\sigma_2}{\sigma_1} \\
                 & = \sigma_2^2 + \rho^2 \sigma_2^2 \\
    p(x_2|x_1) & = N(x_2|\mu_2+\rho\sigma_2^2(x_1-\mu_1), \rho^2\sigma_2^2)
\end{align}

If $\sigma_2 = \sigma_1 = 1$, then

$$p(x_2|x_1) = N(x_2|\mu_2+\rho(x_1-\mu_1), \rho^2)$$

}

\exercise[]{
\textbf{This exercise is shown in the R notebook "ch4-8.ipynb"}}

\exercise[]{
\textbf{Suppose you have two sensors with known (and different) variances
$\nu_1$ and $\nu_2$, but unknown and same mean $\mu$. What is the posterior
$p(\mu|D)$, assuming a non-informative prior for $\mu$?}

In section 4.4.2.1 we saw that the posterior of some observed data from some
noisy measurements of this is given by

\begin{align}
    p(\mu|y_1,y_2,...,y_n) & =  0
    p(\mu|D,\Sigma) & = N(\mu|m_N, V_N) \\
           V_N^{-1} & = V_0^{-1} + N\Sigma^{-1} \\
                m_N & = V_N(\Sigma^{-1}(N\bar{x})+V_0^{-1}m_0) \\
\end{align}

By assuming an uninformative prior, we are saying that $V_0=\infty I$, which
simplifies these to

\begin{align}
    p(\mu|D,\Sigma) & = N(\mu|\bar{x},\frac{1}{N}\Sigma) \\
\end{align}

TODO

}

\exercise[]{
\textbf{Derive the information form results of Section 4.3.1.}

The information form the Gaussian distribution is given by

\begin{align}
    N(x|\xi, \Lambda) & = (2\pi)^{D/2}|\Lambda|^{1/2}exp
   \left [ -\frac{1}{2} (x^T\Lambda x + \xi^T\Lambda^{-1}\xi-2x^T\xi) \right ] \\
    & \propto exp \left [ -\frac{1}{2} (x^T\Lambda x + \xi^T\Lambda^{-1}\xi-2x^T\xi) \right ] \\
    & = exp \left [ -\frac{1}{2}
    \begin{pmatrix}
        x_1 \\ x_2 
    \end{pmatrix}^T
    \begin{pmatrix}
        \Lambda_{11} & \Lambda_{12} \\ 
        \Lambda_{21} & \Lambda_{22} 
    \end{pmatrix}
    \begin{pmatrix}
        x_1\\ x_2 
    \end{pmatrix} +
    \begin{pmatrix}
        \xi_1 \\ \xi_2 
    \end{pmatrix}^T
    \begin{pmatrix}
        \Lambda_{11} & \Lambda_{12} \\ 
        \Lambda_{21} & \Lambda_{22} 
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        \xi_1\\ \xi_2 
    \end{pmatrix} -
    2 \begin{pmatrix}
        x_1 \\ x_2 
    \end{pmatrix}^T
    \begin{pmatrix}
        \xi_1\\ \xi_2 
    \end{pmatrix}
    \right ] \\
    & = exp [ -\frac{1}{2}
    x_2(x_1 \Lambda_{12} + x_2\Lambda_{22}) +
    x_1(x_1\Lambda_{11} + x_2\Lambda_{21}) + \\
    & \begin{pmatrix}
        \xi_1 \\ \xi_2 
    \end{pmatrix}^T
    \begin{pmatrix}
        I & 0 \\ 
        -\Lambda_{22}^{-1}\Lambda_{21} & I 
    \end{pmatrix}
    \begin{pmatrix}
        (\Lambda_{11}-\Lambda_{12}\Lambda_{22}^{-1}\Lambda_{21})^{-1} & 0\\ 
        0 & \Lambda_{22}^{-1}
    \end{pmatrix}
    \begin{pmatrix}
        I & -\Lambda_{12}\Lambda_{22}^{-1} \\ 
        0 & I
    \end{pmatrix}
    \begin{pmatrix}
        \xi_1\\ \xi_2 
    \end{pmatrix}
    - 2x_1 \xi_1 -2x_2 \xi_2 ]
\end{align}

$$N(x|\xi, \Lambda) = N(x|\Sigma^{-1} \mu, \Sigma^{-1})$$

The statements we are trying to prove is

\begin{align}
p(x_1) & = N(x_1|\mu_1, \Sigma_{11}) \\
p(x_2) & = N(x_2|\mu_2, \Sigma_{22}) \\
p(x_1|x_2) & = N(x_1|\mu_{1|2}, \Sigma_{1|2}) \\
\mu_{1|2} & = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2) \\
\Sigma_{1|2} &=\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{align}

}

\exercise[]{
\textbf{Derive equation 4.209}

The posterior is given by

\begin{align}
    p(\mu, \Sigma|D) & = \frac{p(D|\mu, \Sigma)p(\mu, \Sigma)}{p(D)} \\
    & \propto p(D|\mu, \Sigma)NIW(\mu,\Sigma|m_0,\kappa_0,\upsilon_0,S_0) \\
    & = (2\pi)^{ND/2}|\Sigma|^{-\frac{N}{2}}
        exp(-\frac{N}{2}(\mu-\bar{x})^{T}\Sigma^{-1}(\mu-\bar{x})
            -\frac{1}{2}tr(\Sigma^{-1}S_{\bar{x}})) \\
        & \times NIW(\mu,\Sigma|m_0,\kappa_0,\upsilon_0,S_0) \\
    & \propto |\Sigma|^{-\frac{N}{2}}
        exp(-\frac{N}{2}(\mu-\bar{x})^{T}\Sigma^{-1}(\mu-\bar{x})
            -\frac{1}{2}tr(\Sigma^{-1}S_{\bar{x}})) \\
    & \times |\Sigma|^{-\frac{\upsilon_0+D+2}{2}}
        exp(-\frac{\kappa_0}{2}(\mu-m_0)^{T}\Sigma^{-1}(\mu-m_0)
            -\frac{1}{2}tr(\Sigma^{-1}S_0)) \\
    & = |\Sigma|^{-\frac{\upsilon_0+D+2+N}{2}}
        exp(-\frac{N}{2}(\mu-\bar{x})^{T}\Sigma^{-1}(\mu-\bar{x})
            -\frac{\kappa_0}{2}(\mu-m_0)^{T}\Sigma^{-1}(\mu-m_0) \\
    & -\frac{1}{2}tr(\Sigma^{-1}S_0) -\frac{1}{2}tr(\Sigma^{-1}S_{\bar{x}})) \\
    & = |\Sigma|^{-\frac{\upsilon_N+D+2}{2}}
        exp(-\frac{\kappa_N}{2}(\mu-m_N)^{T}\Sigma^{-1}(\mu-m_N)
            -\frac{1}{2}tr(\Sigma^{-1}S_N)) \\
    & = NIW(\mu,\Sigma|m_N,\kappa_N,\upsilon_N,S_N)
\end{align}

}

\exercise[]{
\textbf{a. Derive the BIC score for a Gaussian with dimension $D$ will full
covariance matrix.}

The BIC is given by

\begin{align}
    BIC & = log\,p(D|\hat{\mu},\hat{\Sigma}) - \frac{d}{2}log(N) \\
    & = -\frac{N}{2}tr(\hat{\Sigma}^{-1}\hat{S})
        -\frac{N}{2}log(|\hat{\Sigma}|) - \frac{d}{2}log(N) \\
    & = -\frac{N}{2}tr(\hat{\Sigma}^{-1}\hat{\Sigma})
        -\frac{N}{2}log(|\hat{\Sigma}|) - \frac{d}{2}log(N) \\
    & = -\frac{Nd}{2}-\frac{N}{2}log(|\hat{\Sigma}|) - \frac{d}{2}log(N) \\
    & = -\frac{1}{2}(Nd + dlog(N) + Nlog(|\hat{\Sigma}|))
\end{align}

\textbf{b. Derive the BIC for a Gaussian with diagonal covariance matrix.}

Note that for diagonal matrices, the determinant is just the product of
the diagonals. Thus, we can reduce the above equation to

\begin{align}
    BIC & = log\,p(D|\hat{\mu},\hat{\Sigma}) - \frac{d}{2}log(N) \\
        & = -\frac{1}{2}(Nd + dlog(N) + Nlog(|\hat{\Sigma}|)) \\
        & = -\frac{1}{2}(Nd + dlog(N) + Nlog(\prod_{i=1}^{d} \sigma_i)) \\
        & = -\frac{1}{2}(Nd + dlog(N) + N\sum_{i=1}^{d} log(\sigma_i))
\end{align}

}

\exerciseshere
