\section{Overview of Supervised Learning}

\exercise[]{
\textbf{Suppose each of $K$-classes has an associated target $t_k$, which is a
vector of all zeros, except a one in the $k$th position. Show that classifying
to the largest element of $\hat{y}$ amounts to choosing the closest target,
$min_k ||t_k - \hat{y}||$, if the elements of $\hat{y}$ sum to one.}

We are trying to show that

$$arg\,max_k\,\hat{y_k} = arg\,min_k ||t_k - \hat{y}||$$

To do this, we can show that for any $k^* \neq arg\,max_k\,\hat{y_k}$
and $k = arg\,max_k\,\hat{y_k}$,

$$||t_{k^*} - \hat{y}|| > ||t_{k} - \hat{y}||$$

Note that we can equivalently consider the $||\cdot||^2$ instead of $||\cdot||$,
because they are both monotonic for $\geq 0$.

Using the definition of the Euclidean norm,

\begin{align}
||t_{k^*} - \hat{y}||^2& = ||\hat{y}||^2 + ||t_{k^*}||^2 - 2t_{k^*}\hat{y} \\
                       & = \hat{y}^2 \\
\end{align}

Similarly,

\begin{align}
||t_k - \hat{y}||^2& = ||\hat{y}||^2 + ||t_k||^2 - 2t_k\hat{y} \\
                   & = \hat{y}^2 + 1 - 2\hat{y}
\end{align}

Therefore

\begin{align}
||t_{k^*} - \hat{y}||^2 - ||t_k - \hat{y}||^2& = \hat{y}^2 - (\hat{y}^2 + 1 - 2\hat{y}) \\
                 & = -1 + 2\hat{y} \\
                 & \geq 0
\end{align}

since $\hat{y}$ sums to one. So, $||t_k - \hat{y}||$ is minimized when
$k = arg\,max_k\,\hat{y_k}$.

}

\exercise[]{
Show how to compute the Bayes decision boundary for the simulation example
in Figure 2.5.
}

\exercise[]{
\textbf{Consider N data points uniformly distributed in a p-dimensional unit
ball centered at the origin. Suppose we consider a nearest-neighbor estimate
at the origin. Show that the median distance from the origin to the closest
data point is given by: $d(p, N) = (1 - \frac{1}{2}^{1/N})^{1/p}$.}

Let $m$ be the median distance from the origin to the closest point. This
means that the probability that all data points are further than $m$ is $0.5$.
"Further" simply means a greater norm. Since samples $x_i$ are i.i.d, we can
more formally state this as:

$$\prod_{i=1}^{N} P(||x_i|| > m) = \frac{1}{2}$$

Note that we can flip this around to be
$\prod_{i=1}^{N} P(||x_i|| \leq m) = \frac{1}{2}$.
Now we can use the cumulative function of the uniform distribution as follows:

\begin{align}
\prod_{i=1}^{N} P(||x_i|| \leq m)& = \prod_{i=1}^{N} 1 - ||m|| \\
                                 & = \prod_{i=1}^{N} 1 - m^p \\
                                 & = (1 - m^p)^N = \frac{1}{2}
\end{align}

Now we can solve for $m$:

\begin{align}
\frac{1}{2} & = (1 - m^p)^N \\
\frac{1}{2}^{1/N} & = 1 - m^p \\
m^p & = 1 - \frac{1}{2}^{1/N} \\
m & = (1 - \frac{1}{2}^{1/N})^{1/p}
\end{align}

}

\exercise[]{
\textbf{The edge effect problem discussed on page 23 is not peculiar to uniform
sampling from bounded domains. Consider inputs drawn from a spherical
multinormal distribution $X âˆ¼ N(0, I_p)$. The squared distance from any sample
point to the origin has a $\chi^2_p$ distribution with mean $p$. Consider a
prediction point $x_0$ drawn from this distribution, and let $a = x_0/||x_0||$
be an associated unit vector. Let $z_i = a^Tx_i$ be the projection of each of
the training points on this direction. Show that the $z_i$ are distributed
$N(0, 1)$ with expected squared distance from the origin 1, while the target
point has expected squared distance $p$ from the origin. Hence for $p = 10$,
a randomly drawn test point is about 3.1 standard deviations from the origin,
while all the training points are on average one standard deviation along
direction $a$. So most prediction points see themselves as lying on the edge
of the training set.}

}

\exercise[]{
\textbf{(a) Derive equation (2.27)}

(a) Equation (2.27) states

\begin{align}
EPE(x_0) & = E_{y_0|x_0} E_{\tau} (y_0 - \hat{y}_0)^2 \\
         & = Var(y_0 | x_0) + E_{\tau} [\hat{y}_0 - E_{\tau} \hat{y}_0]^2 + [E_{\tau} \hat{y}_0 - x_0^T\beta]^2 \\
         & = Var(y_0 | x_0) + Var_{\tau}(\hat{y}_0) + Bias^2(\hat{y}_0) \\
         & = \sigma^2 + E_{\tau} x_0^T (\mathbf{X}^T\mathbf{X})^{-1} x_0\sigma^2 + 0^2.
\end{align}

We can derive this is in the three constituent parts shown. Let's start with
the bias squared term. Since we know that the least squares estimator is an
unbiased estimator with respect to the training set, it is obvious that

$$[E_{\tau} \hat{y}_0 - x_0^T\beta]^2 = [x_0^T\beta - x_0^T\beta]^2 = 0^2$$

So, the bias term is zero. Next, we will look at the middle term. Note that
for the least squares solution $\hat{\beta}$,
$Var(\hat{\beta}) = (\mathbf{X}^T\mathbf{X})^{-1} \sigma^2$. So,

\begin{align}
Var_{\tau}(\hat{y}_0) & = Var_{\tau}(x_0^T\hat{\beta}) \\
     & = x_0^2Var(\hat{\beta}) \\
     & = E_{\tau} x_0^T (\mathbf{X}^T\mathbf{X})^{-1} x_0\sigma^2
\end{align}

Finally, it is clear that $Var(y_0 | x_0) = \sigma^2$, since this is how we
define $\sigma^2$.

(b) Equation (2.28) states

\begin{align}
E_{x_0}EPE(x_0) & \sim E_{x_0} x_0^TCov(X)^{-1}x_0\sigma^2 / N + \sigma^2 \\
                & = trace[Cov(X)^{-1}Cov(x_0)]\sigma^2 / N + \sigma^2 \\
                & = \sigma^2(p / N) + \sigma^2
\end{align}

}

\exerciseshere
